{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Quantize Your Language Model\n",
    "\n",
    "### Problem Statement\n",
    "Implement a **language model** using an LSTM and apply **dynamic quantization** to optimize it for inference. Dynamic quantization reduces the model size and enhances inference speed by quantizing the weights of the model.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. **Define the Language Model**:\n",
    "   - **Purpose**: Build a simple language model that predicts the next token in a sequence.\n",
    "   - **Components**:\n",
    "     - **Embedding Layer**: Converts input tokens into dense vector representations.\n",
    "     - **LSTM Layer**: Processes the embedded sequence to capture temporal dependencies.\n",
    "     - **Fully Connected Layer**: Outputs predictions for the next token.\n",
    "     - **Softmax Layer**: Applies a probability distribution over the vocabulary for predictions.\n",
    "   - **Forward Pass**:\n",
    "     - Pass the input sequence through the embedding layer.\n",
    "     - Feed the embedded sequence into the LSTM.\n",
    "     - Use the final hidden state from the LSTM to make predictions via the fully connected layer.\n",
    "     - Apply the softmax function to obtain probabilities over the vocabulary.\n",
    "\n",
    "2. **Apply Dynamic Quantization**:\n",
    "   - Quantize the model dynamically\n",
    "   - Evaluate the quantized model's performance compared to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a simple Language Model (an LSTM-based model)\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use the last hidden state for prediction\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic training data\n",
    "torch.manual_seed(42)\n",
    "vocab_size = 50\n",
    "seq_length = 10\n",
    "batch_size = 32\n",
    "X_train = torch.randint(0, vocab_size, (batch_size, seq_length))  # Random integer input\n",
    "y_train = torch.randint(0, vocab_size, (batch_size,))  # Random target words\n",
    "print(y_train.shape)\n",
    "# Initialize the model, loss function, and optimizer\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "torch.save(model.state_dict(), \"/workspaces/pytorch_handson/easy/data/model.pth\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffineQuantizedTensor(tensor_impl=PlainAQTTensorImpl(data=tensor([[ -19,   62,  105,  ...,   97, -116,  -26],\n",
      "        [  -9,   25,   49,  ...,  -13,  -52,   -1],\n",
      "        [  -6,  -92,   -4,  ...,  -79,  -52,   19],\n",
      "        ...,\n",
      "        [  38, -101,  -31,  ...,  -49,  -58,  126],\n",
      "        [  -6,  -44,   18,  ...,  -70,  -98,  101],\n",
      "        [ -61,  116,  -40,  ...,   72,   70,  -16]], dtype=torch.int8)... , scale=tensor([0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007, 0.0007, 0.0007, 0.0007, 0.0007])... , zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])... , _layout=PlainLayout()), block_size=(1, 128), shape=torch.Size([50, 128]), device=cpu, dtype=torch.float32, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "# for name, param in quantized_model.named_parameters():\n",
    "#     print(name, param.shape, param.dtype)\n",
    "# to check if quanitisation has actuly worked\n",
    "# 1. we should check the by follwing print command, if the type is AffineQuantizedTensor then its quantized\n",
    "print(quantized_model.fc.weight)\n",
    "# not ieven if the weights are quantized they will not be of type torch.qint8\n",
    "# other way to check it is to check the size of the model on the disc before and after quantization\n",
    "\n",
    "# -rw-rw-rw-  1 codespace codespace 946K Dec  3 10:24 model.pth\n",
    "# -rw-rw-rw-  1 codespace codespace 930K Dec  3 10:24 quantized_model.pth\n",
    "# The core reason you didn't see a significant size reduction is likely that the non-quantized parameters and the metadata overhead dominate the file size of your relatively small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 241074\n",
      "Trainable params: 241074\n",
      "Total size: 964296 bytes (941.70KiB)\n"
     ]
    }
   ],
   "source": [
    "# compute total number of parameters and total size in bytes (uses element_size() so dtype-aware)\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.2f}{unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.2f}Pi{suffix}\"\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "print(f\"Total params: {total_params}\")\n",
    "print(f\"Trainable params: {trainable_params}\")\n",
    "print(f\"Total size: {total_bytes} bytes ({sizeof_fmt(total_bytes)})\")\n",
    "\n",
    "# Note the vale count of trainable parametrs chanes after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "Epoch [1/5] - Loss: 3.9118\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "Epoch [2/5] - Loss: 3.9113\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "Epoch [3/5] - Loss: 3.9108\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "Epoch [4/5] - Loss: 3.9103\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n",
      "Epoch [5/5] - Loss: 3.9097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(50, 64)\n",
       "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=50, weight=AffineQuantizedTensor(shape=torch.Size([50, 128]), block_size=(1, 128), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    print(output.shape)\n",
    "    print(y_train.shape)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log progress every epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Now, we will quantize the model dynamically to reduce its size and improve inference speed\n",
    "# Quantization: Apply dynamic quantization to the language model\n",
    "# quantized_model = quantize_dynamic(model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n",
    "from torchao.quantization import quantize_, Int8WeightOnlyConfig\n",
    "config = Int8WeightOnlyConfig()\n",
    "quantized_model = quantize_(model, config)\n",
    "\n",
    "quantize_(model, config) \n",
    "\n",
    "quantized_model = model\n",
    "\n",
    "# Save the quantized model\n",
    "# torch.save(quantized_model.state_dict(), \"quantized_language_model.pth\")\n",
    "quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (embedding): Embedding(50, 64)\n",
      "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=50, weight=AffineQuantizedTensor(shape=torch.Size([50, 128]), block_size=(1, 128), device=cpu, _layout=PlainLayout(), tensor_impl_dtype=torch.int8, quant_min=None, quant_max=None))\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(quantized_model)\n",
    "torch.save(quantized_model.state_dict(), \"/workspaces/pytorch_handson/easy/data/quantized_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 241074\n",
      "Trainable params: 234674\n",
      "Total size: 964296 bytes (941.70KiB)\n"
     ]
    }
   ],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.2f}{unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.2f}Pi{suffix}\"\n",
    "\n",
    "total_params = sum(p.numel() for p in quantized_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad)\n",
    "total_bytes = sum(p.numel() * p.element_size() for p in quantized_model.parameters())\n",
    "\n",
    "print(f\"Total params: {total_params}\")\n",
    "print(f\"Trainable params: {trainable_params}\")\n",
    "print(f\"Total size: {total_bytes} bytes ({sizeof_fmt(total_bytes)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1780/1650460998.py:5: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_model = quantize_dynamic(quantized_model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.ScriptObject was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.ScriptObject])` or the `torch.serialization.safe_globals([torch.ScriptObject])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Apply dynamic quantization on the model after defining it\u001b[39;00m\n\u001b[32m      5\u001b[39m quantized_model = quantize_dynamic(quantized_model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m quantized_model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquantized_language_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.ScriptObject was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.ScriptObject])` or the `torch.serialization.safe_globals([torch.ScriptObject])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Load the quantized model and test it\n",
    "quantized_model = LanguageModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "\n",
    "# Apply dynamic quantization on the model after defining it\n",
    "quantized_model = quantize_dynamic(quantized_model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)\n",
    "\n",
    "quantized_model.load_state_dict(torch.load(\"quantized_language_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for input [[15, 28, 33, 19, 37, 24, 48, 42, 33, 35]]: 49\n"
     ]
    }
   ],
   "source": [
    "# Testing the quantized model on a sample input\n",
    "quantized_model.eval()\n",
    "test_input = torch.randint(0, vocab_size, (1, seq_length))\n",
    "with torch.no_grad():\n",
    "    prediction = quantized_model(test_input)\n",
    "    print(f\"Prediction for input {test_input.tolist()}: {prediction.argmax(dim=1).item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
